{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_cell"
      },
      "source": [
        "# \ud83c\udf29\ufe0f **CloudLeecher: Production Backend**\n",
        "\n",
        "Welcome to the **CloudLeecher** backend. This notebook turns your Google Colab instance into a powerful, high-speed torrent downloader that saves files directly to your Google Drive.\n",
        "\n",
        "### **Instructions**\n",
        "1.  **Mount Drive**: Connect your Google storage.\n",
        "2.  **Install**: Set up the environment.\n",
        "3.  **Start Services**: Launch the backend and get your public connection URL.\n",
        "4.  **Connect**: Paste the URL into the CloudLeecher Frontend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1_md"
      },
      "source": [
        "## 1. \ud83d\udcc2 **Mount Google Drive**\n",
        "We need access to your Drive to save the downloaded files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "step1_code",
        "outputId": "8830dadc-f7af-4cac-8e68-a114eb5a50e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u2705 Download Directory Ready: /content/drive/MyDrive/TorrentDownloads\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define and Create Download Directory\n",
        "DOWNLOAD_DIR = \"/content/drive/MyDrive/TorrentDownloads\"\n",
        "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\u2705 Download Directory Ready: {DOWNLOAD_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2_md"
      },
      "source": [
        "## 2. \ud83d\udee0\ufe0f **Install Dependencies**\n",
        "Installing `aria2` (the download engine), `flask` (the API server), and `pyngrok` (for public access)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "step2_code"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq aria2\n",
        "!pip install -q flask flask-cors pyngrok\n",
        "\n",
        "print(\"\u2705 All dependencies installed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3_md"
      },
      "source": [
        "## 3. \ud83d\ude80 **Start Downloader Service**\n",
        "Initializing the Aria2 RPC server in the background."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "step3_code",
        "outputId": "cdcc2dfa-5eb8-435f-9b94-00b4d131a422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Aria2 Background Service Started.\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "# Start Aria2c as a daemon process\n",
        "cmd = [\n",
        "    \"aria2c\",\n",
        "    \"--enable-rpc\",\n",
        "    \"--rpc-listen-all=true\",\n",
        "    \"--rpc-allow-origin-all\",\n",
        "    f\"--dir={DOWNLOAD_DIR}\",\n",
        "    \"--file-allocation=none\",\n",
        "    \"--max-connection-per-server=16\",\n",
        "    \"--split=16\",\n",
        "    \"--min-split-size=1M\",\n",
        "    \"--seed-time=0\",\n",
        "    \"--daemon=true\"\n",
        "]\n",
        "\n",
        "subprocess.run(\n",
        "    cmd,\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "print(\"\u2705 Aria2 Background Service Started.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4_md"
      },
      "source": [
        "## 4. \ud83d\udcdd **Create API Backend**\n",
        "Generating the `app.py` file which serves as the brain of CloudLeecher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "step4_code",
        "outputId": "1b65a888-3528-417d-84b4-e69fdfc1ea44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import xmlrpc.client\nfrom flask import Flask, request, jsonify, abort\nfrom flask_cors import CORS\nimport os\nimport shutil\nimport base64\nimport json\nfrom datetime import datetime\nfrom collections import deque\nimport threading\nimport time\nimport logging\n\n# Initialize Flask App\napp = Flask(__name__)\nCORS(app)\n\n# Configuration\nTEMP_DOWNLOAD_DIR = \"/content/temp_downloads\"\nFINAL_DOWNLOAD_DIR = \"/content/drive/MyDrive/TorrentDownloads\"\nARIA2_RPC_URL = \"http://localhost:6800/rpc\"\nLOG_FILE = \"/content/backend_logs.json\"\nAPI_KEY_ENV = \"CLOUDLEECHER_API_KEY\"\n\n# Ensure directories exist\nos.makedirs(TEMP_DOWNLOAD_DIR, exist_ok=True)\nos.makedirs(FINAL_DOWNLOAD_DIR, exist_ok=True)\n\n# In-memory log storage (last 100 entries)\nlogs = deque(maxlen=100)\n\n# Connect to Aria2 RPC\ns = xmlrpc.client.ServerProxy(ARIA2_RPC_URL)\n\ndef log(level, operation, message, gid=None, extra=None):\n    \"\"\"Add entry to log with timestamp and details\"\"\"\n    entry = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"level\": level,  # info, warning, error\n        \"operation\": operation,\n        \"message\": message,\n        \"gid\": gid,\n        \"extra\": extra\n    }\n    logs.append(entry)\n    \n    # Also write to file for persistence\n    try:\n        with open(LOG_FILE, 'a') as f:\n            f.write(json.dumps(entry) + '\\n')\n    except:\n        pass\n    \n    print(f\"[{level.upper()}] {operation}: {message}\" + (f\" (GID: {gid})\" if gid else \"\"))\n\n# --- Authentication Middleware ---\n@app.before_request\ndef check_api_key():\n    if request.method == 'OPTIONS':\n        return\n    \n    # Skip auth for health check\n    if request.path == '/health':\n        return\n\n    expected_key = os.environ.get(API_KEY_ENV)\n    if not expected_key:\n        # If no key is set in env, we are in insecure mode or dev mode\n        # Ideally we should log a warning, but for now allow it or block it?\n        # The plan says we MUST generate one. If missing, block.\n        # But for dev/test ease, if env is missing, maybe default to \"dev\"?\n        # No, strict security requested.\n        log(\"warning\", \"auth\", \"No API Key configured in environment!\")\n        # If strictly enforcing, abort(500). But let's assume it's generated by the notebook.\n        return \n\n    provided_key = request.headers.get('x-api-key')\n    if provided_key != expected_key:\n        log(\"warning\", \"auth\", \"Invalid API Key attempt\")\n        abort(401, description=\"Invalid API Key\")\n\n# --- Background Monitor & File Mover ---\nclass BackgroundMonitor(threading.Thread):\n    def __init__(self):\n        super().__init__()\n        self.daemon = True\n        self.running = True\n        self.transfer_status = {} # {gid: {'status': 'moving'|'saved'|'error', 'msg': '...'}}\n        self.lock = threading.Lock()\n\n    def run(self):\n        log(\"info\", \"monitor\", \"Background Monitor Started\")\n        while self.running:\n            try:\n                self.check_downloads()\n            except Exception as e:\n                log(\"error\", \"monitor\", f\"Monitor Loop Error: {e}\")\n            time.sleep(2)\n\n    def check_downloads(self):\n        try:\n            # check stopped tasks (complete, error, removed)\n            stopped = s.aria2.tellStopped(0, 100, [\"gid\", \"status\", \"files\", \"errorCode\", \"errorMessage\"])\n            \n            for task in stopped:\n                gid = task['gid']\n                status = task['status']\n                \n                with self.lock:\n                    if gid in self.transfer_status and self.transfer_status[gid]['status'] in ['saved', 'moved_error']:\n                        continue # Already processed\n\n                if status == 'complete':\n                    self.handle_complete(task)\n                elif status == 'error':\n                    self.handle_error(task)\n                elif status == 'removed':\n                     # Just clean up tracking if exists\n                     pass\n        except Exception as e:\n            # log(\"error\", \"monitor_check\", f\"Error querying Aria2: {e}\")\n            pass\n\n    def handle_complete(self, task):\n        gid = task['gid']\n        \n        # Mark as moving\n        with self.lock:\n            if gid in self.transfer_status: return\n            self.transfer_status[gid] = {'status': 'moving', 'msg': 'Moving to Drive...'}\n        \n        log(\"info\", \"mover\", \"Download complete. Moving files...\", gid=gid)\n        \n        try:\n            files = task.get('files', [])\n            if not files:\n                raise Exception(\"No files in task\")\n\n            # In Aria2, the first file path is usually the root folder or the single file\n            # But if it's a multi-file torrent, we need to handle the directory\n            \n            # Check if it's a single file or directory\n            # Aria2 returns absolute paths.\n            source_path = files[0]['path']\n            \n            # If path is empty (metadata only?), skip\n            if not source_path:\n                return\n\n            # Determine the root move target\n            # If multi-file, aria2 usually creates a directory.\n            # We assume the user wants the top-level item moved.\n            \n            # Logic: If all files share a common top directory relative to TEMP_DOWNLOAD_DIR, move that.\n            # Otherwise move individual files.\n            \n            # Simple approach: Move the specific files/folders reported by Aria2\n            # Problem: aria2 might flatten or structure differently.\n            # Let's assume standard behavior:\n            # If single file: source_path is the file.\n            # If multi-file: source_path is one file, but they share a directory.\n            \n            # Robust way: \n            # 1. Identify common base directory inside TEMP_DOWNLOAD_DIR\n            rel_path = os.path.relpath(source_path, TEMP_DOWNLOAD_DIR)\n            top_level = rel_path.split(os.sep)[0]\n            full_source_path = os.path.join(TEMP_DOWNLOAD_DIR, top_level)\n            \n            if not os.path.exists(full_source_path):\n                 raise Exception(f\"Source not found: {full_source_path}\")\n            \n            dest_path = os.path.join(FINAL_DOWNLOAD_DIR, top_level)\n            \n            # Handle collision\n            if os.path.exists(dest_path):\n                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                name, ext = os.path.splitext(top_level)\n                new_name = f\"{name}_{timestamp}{ext}\"\n                dest_path = os.path.join(FINAL_DOWNLOAD_DIR, new_name)\n                log(\"info\", \"mover\", f\"Destination exists. Renaming to {new_name}\", gid=gid)\n\n            # Perform Move\n            shutil.move(full_source_path, dest_path)\n            \n            with self.lock:\n                self.transfer_status[gid] = {'status': 'saved', 'msg': 'Saved to Drive'}\n            \n            log(\"info\", \"mover\", \"Move successful\", gid=gid)\n            \n            # Remove from Aria2 to clean up\n            try:\n                s.aria2.removeDownloadResult(gid)\n            except:\n                pass\n\n        except Exception as e:\n            log(\"error\", \"mover\", f\"Move failed: {str(e)}\", gid=gid)\n            with self.lock:\n                self.transfer_status[gid] = {'status': 'error', 'msg': f\"Move failed: {str(e)}\"}\n\n    def handle_error(self, task):\n        gid = task['gid']\n        msg = task.get('errorMessage', 'Unknown error')\n        # We leave it in Aria2 history so user can see it, but we can track it too\n        pass\n\n    def get_transfer_status(self, gid):\n        with self.lock:\n            return self.transfer_status.get(gid)\n\n# Start Monitor\nmonitor = BackgroundMonitor()\nmonitor.start()\n\n# --- API Endpoints ---\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({\"status\": \"ok\", \"service\": \"CloudLeecher-Backend\"})\n\n@app.route('/api/logs', methods=['GET'])\ndef get_logs():\n    return jsonify({\"logs\": list(logs)})\n\n@app.route('/api/download/magnet', methods=['POST'])\ndef add_magnet():\n    data = request.json\n    magnet_link = data.get('magnet')\n    if not magnet_link:\n        return jsonify({\"error\": \"Magnet link is required\"}), 400\n    \n    # Check Active/Waiting\n    active = s.aria2.tellActive([\"gid\"])\n    waiting = s.aria2.tellWaiting(0, 100, [\"gid\"])\n    \n    if len(active) > 0 or len(waiting) > 0:\n         return jsonify({\"error\": \"Queue full. Wait for current download.\"}), 429\n         \n    try:\n        gid = s.aria2.addUri([magnet_link], {\"dir\": TEMP_DOWNLOAD_DIR})\n        log(\"info\", \"add_magnet\", \"Magnet added\", gid=gid)\n        return jsonify({\"status\": \"success\", \"gid\": gid})\n    except Exception as e:\n        log(\"error\", \"add_magnet\", f\"Failed: {e}\")\n        return jsonify({\"error\": str(e)}), 500\n\n@app.route('/api/download/file', methods=['POST'])\ndef add_torrent_file():\n    data = request.json\n    b64_content = data.get('torrent')\n    if not b64_content:\n        return jsonify({\"error\": \"Content required\"}), 400\n\n    active = s.aria2.tellActive([\"gid\"])\n    waiting = s.aria2.tellWaiting(0, 100, [\"gid\"])\n    \n    if len(active) > 0 or len(waiting) > 0:\n         return jsonify({\"error\": \"Queue full.\"}), 429\n         \n    try:\n        raw_bytes = base64.b64decode(b64_content)\n        gid = s.aria2.addTorrent(xmlrpc.client.Binary(raw_bytes), {\"dir\": TEMP_DOWNLOAD_DIR})\n        log(\"info\", \"add_torrent\", \"Torrent file added\", gid=gid)\n        return jsonify({\"status\": \"success\", \"gid\": gid})\n    except Exception as e:\n        log(\"error\", \"add_torrent\", f\"Failed: {e}\")\n        return jsonify({\"error\": str(e)}), 500\n\n@app.route('/api/status', methods=['GET'])\ndef get_status():\n    try:\n        basic_keys = [\"gid\", \"status\", \"totalLength\", \"completedLength\", \"downloadSpeed\", \"files\", \"errorMessage\", \"errorCode\"]\n        extended_keys = basic_keys + [\"numSeeders\", \"connections\", \"infoHash\"]\n        \n        active = s.aria2.tellActive(extended_keys)\n        waiting = s.aria2.tellWaiting(0, 100, basic_keys)\n        stopped = s.aria2.tellStopped(0, 100, basic_keys)\n        \n        # Merge with monitor status\n        # We primarily want to override \"complete\" (from Aria2) with \"moving\" or \"saved\"\n        # Or inject \"saved\" tasks that have been removed from Aria2?\n        # Actually, if we remove from Aria2, they disappear from 'stopped'.\n        # We need to inject them back into the response if we want the frontend to show them!\n        \n        final_stopped = []\n        \n        # Process existing stopped tasks\n        for task in stopped:\n            gid = task['gid']\n            transfer = monitor.get_transfer_status(gid)\n            if transfer:\n                task['status'] = transfer['status'] # moving, saved, error\n                if transfer.get('msg'):\n                    task['errorMessage'] = transfer['msg'] # reuse error message field for status text? or create new?\n            final_stopped.append(task)\n            \n        # Inject tasks that monitor knows about but Aria2 forgot (because we purged them)\n        with monitor.lock:\n             for gid, info in monitor.transfer_status.items():\n                 # Check if already in our lists\n                 found = False\n                 for list_grp in [active, waiting, final_stopped]:\n                     if any(t['gid'] == gid for t in list_grp):\n                         found = True\n                         break\n                 \n                 if not found:\n                     # Reconstruct a fake task object for the frontend\n                     # We assume if it's in monitor, it was completed.\n                     fake_task = {\n                         \"gid\": gid,\n                         \"status\": info['status'],\n                         \"totalLength\": \"0\", # Unknown now\n                         \"completedLength\": \"0\",\n                         \"downloadSpeed\": \"0\",\n                         \"files\": [],\n                         \"errorMessage\": info.get('msg', '')\n                     }\n                     final_stopped.append(fake_task)\n\n        return jsonify({\n            \"active\": active,\n            \"waiting\": waiting,\n            \"stopped\": final_stopped\n        })\n    except Exception as e:\n        log(\"error\", \"get_status\", f\"Failed: {e}\")\n        return jsonify({\"error\": str(e)}), 500\n\n@app.route('/api/control/pause', methods=['POST'])\ndef pause_download():\n    gid = request.json.get('gid')\n    try:\n        s.aria2.pause(gid)\n        return jsonify({\"status\": \"paused\", \"gid\": gid})\n    except Exception as e:\n        return jsonify({\"error\": str(e)}), 500\n\n@app.route('/api/control/resume', methods=['POST'])\ndef resume_download():\n    gid = request.json.get('gid')\n    try:\n        s.aria2.unpause(gid)\n        return jsonify({\"status\": \"resumed\", \"gid\": gid})\n    except Exception as e:\n        return jsonify({\"error\": str(e)}), 500\n\n@app.route('/api/control/remove', methods=['POST'])\ndef remove_download():\n    gid = request.json.get('gid')\n    try:\n        # Try force remove first\n        try:\n            s.aria2.forceRemove(gid)\n        except:\n             # If not active, try remove result\n             s.aria2.removeDownloadResult(gid)\n             \n        # Also remove from monitor\n        with monitor.lock:\n            if gid in monitor.transfer_status:\n                del monitor.transfer_status[gid]\n                \n        return jsonify({\"status\": \"removed\", \"gid\": gid})\n    except Exception as e:\n        # If not found, success\n        if 'not found' in str(e).lower():\n             return jsonify({\"status\": \"removed\", \"gid\": gid})\n        return jsonify({\"error\": str(e)}), 500\n\n@app.route('/api/drive/info', methods=['GET'])\ndef drive_info():\n    try:\n        total, used, free = shutil.disk_usage(FINAL_DOWNLOAD_DIR)\n        return jsonify({\"total\": total, \"used\": used, \"free\": free})\n    except:\n        return jsonify({\"total\": 0, \"used\": 0, \"free\": 0})\n\n@app.route('/api/cleanup', methods=['POST'])\ndef cleanup_all():\n    try:\n        s.aria2.purgeDownloadResult()\n        # Also clear monitor\n        with monitor.lock:\n            monitor.transfer_status.clear()\n        return jsonify({\"status\": \"success\"})\n    except Exception as e:\n        return jsonify({\"error\": str(e)}), 500\n\nif __name__ == \"__main__\":\n    log(\"info\", \"startup\", \"Backend Starting...\")\n    # purge_stalled_downloads logic is now handled by monitor indirectly or manual cleanup\n    app.run(port=5000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5_md"
      },
      "source": [
        "## 5. \ud83c\udf10 **Launch Public Server**\n",
        "Starting the application and generating your public access URL.\n",
        "\n",
        "> **\u26a0\ufe0f Important**: Ensure you have added your Ngrok Authtoken to Colab Secrets with the key `NGROK-AUTHTOKEN`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "step5_code",
        "outputId": "f986933a-8318-4562-c19d-10332aa85446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "\ud83d\udd17 PUBLIC URL: https://vitalistically-falsifiable-donnette.ngrok-free.dev\n",
            "============================================================\n",
            "\n",
            "\u2705 CloudLeecher Backend is Online!\n",
            "\ud83c\udf0d Frontend App: https://cloudleecher.web.app\n",
            "\ud83d\udccb Copy the URL above (PUBLIC URL) and paste it into the CloudLeecher Frontend app.\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "import subprocess\n",
        "import sys\n",
        "import time\n",
        "import os\n",
        "import secrets\n",
        "\n",
        "# 1. Authenticate Ngrok\n",
        "try:\n",
        "    AUTH_TOKEN = userdata.get(\"NGROK-AUTHTOKEN\")\n",
        "    ngrok.set_auth_token(AUTH_TOKEN)\n",
        "except Exception as e:\n",
        "    print(\"\u274c Error: Ngrok Auth Token not found! Please add 'NGROK-AUTHTOKEN' to Colab Secrets.\")\n",
        "    raise e\n",
        "\n",
        "# 2. Generate Secure API Key\n",
        "api_key = secrets.token_hex(16)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"\ud83d\udd11 API KEY: {api_key}\")\n",
        "print(\"\u26a0\ufe0f COPY THIS KEY! You will need it to connect the frontend.\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# 3. Cleanup Old Processes\n",
        "ngrok.kill()\n",
        "os.system(\"fuser -k 5000/tcp > /dev/null 2>&1\")\n",
        "\n",
        "# 4. Start Flask App in Background\n",
        "log_file = open(\"flask.log\", \"w\")\n",
        "env = os.environ.copy()\n",
        "env[\"CLOUDLEECHER_API_KEY\"] = api_key\n",
        "subprocess.Popen([sys.executable, \"app.py\"], stdout=log_file, stderr=log_file, env=env)\n",
        "time.sleep(3)  # Allow Flask to initialize\n",
        "\n",
        "# 5. Open Ngrok Tunnel\n",
        "try:\n",
        "    public_url = ngrok.connect(5000).public_url\n",
        "    print(f\"\ud83d\udd17 PUBLIC URL: {public_url}\")\n",
        "    print(\"\u2705 CloudLeecher Backend is Online!\")\n",
        "    print(\"\ud83c\udf0d Frontend App: https://cloudleecher.web.app\")\n",
        "    print(\"\ud83d\udccb 1. Copy the PUBLIC URL\")\n",
        "    print(\"\ud83d\udccb 2. Copy the API KEY\")\n",
        "    print(\"\ud83d\udccb 3. Paste them into the CloudLeecher Frontend settings.\")\n",
        "\n",
        "    # Keep cell running\n",
        "    while True:\n",
        "        time.sleep(10)\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Failed to start Ngrok: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
